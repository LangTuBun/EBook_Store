{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "PROBABILITIES = [0.005, 0.05, 0.1, 0.4, 0.445]\n",
    "\n",
    "def download_image(source, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        image_content = requests.get(source).content\n",
    "        f.write(image_content)\n",
    "        print('Downloaded image at:', filename)\n",
    "        \n",
    "\n",
    "class OpenLibraryScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.books_data = []\n",
    "        self.base_url = \"https://openlibrary.org\"\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"Make a request to the URL and return BeautifulSoup object\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "            return None\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean extracted text by removing extra whitespace and newlines\"\"\"\n",
    "        if text:\n",
    "            return re.sub(r'\\s+', ' ', text).strip()\n",
    "        return ''\n",
    "\n",
    "    def scrape_book(self, url):\n",
    "        \"\"\"Scrape individual book page from Open Library\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            book_data = {\n",
    "                'url': url,\n",
    "                'title': '',\n",
    "                'authors': [],\n",
    "                'publisher': '',\n",
    "                'publish_date': '',\n",
    "                'category': [],\n",
    "                'rating': '',\n",
    "                'page_count': '',\n",
    "                'languages': [],\n",
    "                'image_url': ''\n",
    "            }\n",
    "\n",
    "         \n",
    "\n",
    "            # Title\n",
    "            title_elem = soup.select_one('h1.work-title')\n",
    "            if title_elem:\n",
    "                book_data['title'] = self.clean_text(title_elem.text)\n",
    "            # Image URL\n",
    "            img_elem = soup.find('img', attrs = {'itemprop': 'image'})\n",
    "            img_url = 'https:' + img_elem['src'] if img_elem else None\n",
    "            valid_img = re.search(r\"openlibrary.org\", img_url)\n",
    "            if valid_img:\n",
    "                book_data['image_url'] = img_url\n",
    "                download_image(img_url, f\"images/{book_data['title']}.jpg\")\n",
    "            else:\n",
    "                print('No image found for:', book_data['title'])\n",
    "                return None\n",
    "            rating = random.choices([1,2,3,4,5], PROBABILITIES, k=1)[0]\n",
    "            book_data['rating'] = rating\n",
    "            # Authors\n",
    "            author_elems = soup.select('a[href^=\"/author\"]')\n",
    "            book_data['authors'] = list(set([self.clean_text(author.text) for author in author_elems]))\n",
    "\n",
    "            # Publisher and publish date\n",
    "            publish_info = soup.select('div.edition-omniline')\n",
    "            if publish_info:\n",
    "                pub_text = publish_info[0].text\n",
    "                pub_text = self.clean_text(pub_text)\n",
    "                publish_date_pattern = r\"Publish Date (\\w* \\d{1,2}, )?(?P<publish_date>\\d{4})\"\n",
    "\n",
    "                publisher_pattern = r\"Publisher (?P<publisher>.*) [Language]?\"  # Extract publisher\n",
    "                pages_pattern = r\"Pages (?P<pages>\\d+)\"  # Extract pages\n",
    "                language_pattern = r\"Language (?P<language>[^\\s]+)\"  # Extract language\n",
    "                # Extract publisher\n",
    "                # match = re.search(pattern, pub_text)\n",
    "                publisher_match = re.search(publisher_pattern, pub_text).group('publisher') if re.search(publisher_pattern, pub_text) else None\n",
    "                date_match = re.search(publish_date_pattern, pub_text).group('publish_date') if re.search(publish_date_pattern, pub_text) else None\n",
    "                pages_match = re.search(pages_pattern, pub_text).group('pages') if re.search(pages_pattern, pub_text) else None\n",
    "                language_match = re.search(language_pattern, pub_text).group('language') if re.search(language_pattern, pub_text) else None \n",
    "                if publisher_match:\n",
    "                    book_data['publisher'] = self.clean_text(publisher_match)\n",
    "                if date_match:\n",
    "                    book_data['publish_date'] = date_match\n",
    "                if language_match:\n",
    "                    book_data['languages'] = language_match\n",
    "                if pages_match:\n",
    "                    book_data['page_count'] = pages_match     \n",
    "\n",
    "            # category\n",
    "            subject_elems = soup.select('a[href^=\"/subjects/\"]')\n",
    "            book_data['category'] = [self.clean_text(subject.text) for subject in subject_elems]\n",
    "\n",
    "            # Description\n",
    "            desc_elem = soup.select_one('div[itemprop=\"description\"]')\n",
    "            if desc_elem:\n",
    "                book_data['description'] = self.clean_text(desc_elem.text)\n",
    "\n",
    "            # Page count\n",
    "            pages_elem = soup.select_one('span[itemprop=\"numberOfPages\"]')\n",
    "            if pages_elem:\n",
    "                book_data['page_count'] = self.clean_text(pages_elem.text)\n",
    "\n",
    "\n",
    "        \n",
    "            self.books_data.append(book_data)\n",
    "            return book_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping book {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def search_books(self, query, max_results=5):\n",
    "        \"\"\"Search for books and return their URLs\"\"\"\n",
    "        search_url = f\"{self.base_url}/search?q={query.replace(' ', '+')}&mode=everything\"\n",
    "        soup = self.get_soup(search_url)\n",
    "        if not soup:\n",
    "            return []\n",
    "\n",
    "        book_links = []\n",
    "        results = soup.select('h3.booktitle a[href^=\"/works/\"]')\n",
    "        \n",
    "        for result in results[:max_results]:\n",
    "            book_url = self.base_url + result['href']\n",
    "            book_links.append(book_url)\n",
    "            \n",
    "        return book_links\n",
    "\n",
    "    def scrape_books(self, urls):\n",
    "        \"\"\"Scrape multiple book pages\"\"\"\n",
    "        i = 0\n",
    "        for url in urls:\n",
    "            i+=1\n",
    "            print(i)\n",
    "            print(f\"Scraping: {url}\")\n",
    "            self.scrape_book(url)\n",
    "            sleep(2)  # Be nice to the server\n",
    "        \n",
    "        return self.export_to_dataframe()\n",
    "\n",
    "    def export_to_dataframe(self):\n",
    "        \"\"\"Export scraped data to pandas DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.books_data)\n",
    "\n",
    "    def save_to_csv(self, filename=\"books_data.csv\"):\n",
    "        \"\"\"Save scraped data to CSV file\"\"\"\n",
    "        df = self.export_to_dataframe()\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraping the URL of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://openlibrary.org/trending/now?page=1\n",
      "Scraping page 2: https://openlibrary.org/trending/now?page=2\n",
      "Scraping page 3: https://openlibrary.org/trending/now?page=3\n",
      "Scraping page 4: https://openlibrary.org/trending/now?page=4\n",
      "Scraping page 5: https://openlibrary.org/trending/now?page=5\n",
      "Scraping page 6: https://openlibrary.org/trending/now?page=6\n",
      "Scraping page 7: https://openlibrary.org/trending/now?page=7\n",
      "Scraping page 8: https://openlibrary.org/trending/now?page=8\n",
      "Scraping page 9: https://openlibrary.org/trending/now?page=9\n",
      "Scraping page 10: https://openlibrary.org/trending/now?page=10\n",
      "Scraping page 11: https://openlibrary.org/trending/now?page=11\n",
      "Scraping page 12: https://openlibrary.org/trending/now?page=12\n",
      "Scraping page 13: https://openlibrary.org/trending/now?page=13\n",
      "Scraping page 14: https://openlibrary.org/trending/now?page=14\n",
      "Scraping page 15: https://openlibrary.org/trending/now?page=15\n",
      "Scraping page 16: https://openlibrary.org/trending/now?page=16\n",
      "Scraping page 17: https://openlibrary.org/trending/now?page=17\n",
      "Scraping page 18: https://openlibrary.org/trending/now?page=18\n",
      "Scraping page 19: https://openlibrary.org/trending/now?page=19\n",
      "Scraping page 20: https://openlibrary.org/trending/now?page=20\n",
      "Scraping page 21: https://openlibrary.org/trending/now?page=21\n",
      "Scraping page 22: https://openlibrary.org/trending/now?page=22\n",
      "Scraping page 23: https://openlibrary.org/trending/now?page=23\n",
      "Scraping page 24: https://openlibrary.org/trending/now?page=24\n",
      "Scraping page 25: https://openlibrary.org/trending/now?page=25\n",
      "Scraping page 26: https://openlibrary.org/trending/now?page=26\n",
      "Scraping page 27: https://openlibrary.org/trending/now?page=27\n",
      "Scraping page 28: https://openlibrary.org/trending/now?page=28\n",
      "Scraping page 29: https://openlibrary.org/trending/now?page=29\n",
      "Scraping page 30: https://openlibrary.org/trending/now?page=30\n",
      "Scraping page 31: https://openlibrary.org/trending/now?page=31\n",
      "Scraping page 32: https://openlibrary.org/trending/now?page=32\n",
      "Scraping page 33: https://openlibrary.org/trending/now?page=33\n",
      "Scraping page 34: https://openlibrary.org/trending/now?page=34\n",
      "Scraping page 35: https://openlibrary.org/trending/now?page=35\n",
      "Scraping page 36: https://openlibrary.org/trending/now?page=36\n",
      "Scraping page 37: https://openlibrary.org/trending/now?page=37\n",
      "Scraping page 38: https://openlibrary.org/trending/now?page=38\n",
      "Scraping page 39: https://openlibrary.org/trending/now?page=39\n",
      "Scraping page 40: https://openlibrary.org/trending/now?page=40\n",
      "Scraping page 41: https://openlibrary.org/trending/now?page=41\n",
      "Scraping page 42: https://openlibrary.org/trending/now?page=42\n",
      "Scraping page 43: https://openlibrary.org/trending/now?page=43\n",
      "Scraping page 44: https://openlibrary.org/trending/now?page=44\n",
      "Scraping page 45: https://openlibrary.org/trending/now?page=45\n",
      "Scraping page 46: https://openlibrary.org/trending/now?page=46\n",
      "Scraping page 47: https://openlibrary.org/trending/now?page=47\n",
      "Scraping page 48: https://openlibrary.org/trending/now?page=48\n",
      "Scraping page 49: https://openlibrary.org/trending/now?page=49\n",
      "Scraping page 50: https://openlibrary.org/trending/now?page=50\n",
      "Data saved to python_book_urls.csv\n",
      "Found 1000 book URLs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "class OpenLibraryURLScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.book_urls = []\n",
    "        self.base_url = \"https://openlibrary.org\"\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"Make a request to the URL and return BeautifulSoup object\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_book_urls(self, query, max_pages=10):\n",
    "        \"\"\"Search for books and extract all book URLs\"\"\"\n",
    "        for page_num in range(1, max_pages + 1):\n",
    "            search_url = f\"{self.base_url}/trending/now?page={page_num}\"\n",
    "            print(f\"Scraping page {page_num}: {search_url}\")\n",
    "            soup = self.get_soup(search_url)\n",
    "            if not soup:\n",
    "                break\n",
    "\n",
    "            book_links = soup.select('h3.booktitle a[href^=\"/works/\"]')\n",
    "            for link in book_links:\n",
    "                book_url = self.base_url + link['href']\n",
    "                self.book_urls.append(book_url)\n",
    "\n",
    "            # Be nice to the server\n",
    "            sleep(1)\n",
    "\n",
    "        return self.book_urls\n",
    "\n",
    "    def export_to_dataframe(self):\n",
    "        \"\"\"Export scraped URLs to pandas DataFrame\"\"\"\n",
    "        return pd.DataFrame({'url': self.book_urls})\n",
    "\n",
    "    def save_to_csv(self, filename=\"book_urls.csv\"):\n",
    "        \"\"\"Save scraped URLs to CSV file\"\"\"\n",
    "        df = self.export_to_dataframe()\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = OpenLibraryURLScraper()\n",
    "    search_query = \"python programming\"\n",
    "    book_urls = scraper.scrape_book_urls(search_query, max_pages=50)\n",
    "    scraper.save_to_csv(\"python_book_urls.csv\")\n",
    "    print(f\"Found {len(book_urls)} book URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://openlibrary.org/works/OL1968368W?edition=ia%3Apowerdie48gesetz0000robe', 'https://openlibrary.org/works/OL15987761W?edition=ia%3Ablueumbrellanove0000maso', 'https://openlibrary.org/works/OL21650110W', 'https://openlibrary.org/works/OL37926596W', 'https://openlibrary.org/works/OL15844502W?edition=ia%3Apathstorecoverya00alan']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"python_book_urls.csv\")\n",
    "book_urls = df['url'].tolist()\n",
    "print(book_urls[:5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Scraping: https://openlibrary.org/works/OL25421708W\n",
      "No image found for: Advancing Equity and Embracing Diversity in Early Childhood Education\n",
      "2\n",
      "Scraping: https://openlibrary.org/works/OL18593468W?edition=ia%3Afriend0000hill\n",
      "Downloaded image at: images/Friend.jpg\n",
      "3\n",
      "Scraping: https://openlibrary.org/works/OL3098719W?edition=ia%3Acompletedistance0000tull\n",
      "Downloaded image at: images/The complete distance runner.jpg\n",
      "4\n",
      "Scraping: https://openlibrary.org/works/OL17364743W?edition=ia%3Ahiddenoracle0000rior_l5g3\n",
      "Downloaded image at: images/The Hidden Oracle.jpg\n",
      "5\n",
      "Scraping: https://openlibrary.org/works/OL16808392W?edition=ia%3Ahouseofhades0000rior_f4t9\n",
      "Downloaded image at: images/The house of Hades.jpg\n",
      "6\n",
      "Scraping: https://openlibrary.org/works/OL16664287W?edition=ia%3Alamarcadeatenea0000rior\n",
      "Downloaded image at: images/La marca de Atenea.jpg\n",
      "7\n",
      "Scraping: https://openlibrary.org/works/OL15401200W?edition=ia%3Aelheroeperdido0000rior\n",
      "Downloaded image at: images/El héroe perdido.jpg\n",
      "8\n",
      "Scraping: https://openlibrary.org/works/OL17503772W?edition=ia%3Aisbn_9780982841488\n",
      "Downloaded image at: images/The Pussy Trap A Novel.jpg\n",
      "9\n",
      "Scraping: https://openlibrary.org/works/OL17448397W?edition=ia%3Amagicmadnessultr0000moun\n",
      "Downloaded image at: images/Magic Madness And Ultramarathon Running.jpg\n",
      "10\n",
      "Scraping: https://openlibrary.org/works/OL80300W?edition=ia%3Aultramarathonswo00aase\n",
      "Downloaded image at: images/Ultramarathons.jpg\n",
      "11\n",
      "Scraping: https://openlibrary.org/works/OL1958646W?edition=ia%3Abasiccomposition00huiz\n",
      "Downloaded image at: images/Basic composition for ESL.jpg\n",
      "12\n",
      "Scraping: https://openlibrary.org/works/OL1790228W?edition=ia%3Astruggleformozam0000mond\n",
      "Downloaded image at: images/The struggle for Mozambique.jpg\n",
      "13\n",
      "Scraping: https://openlibrary.org/works/OL257943W?edition=ia%3Aletrnedeferlintg0000mart\n",
      "Downloaded image at: images/Le trône de fer.jpg\n",
      "14\n",
      "Scraping: https://openlibrary.org/works/OL16458594W?edition=ia%3Atechnicalanalysi00kauf\n",
      "Downloaded image at: images/Technical analysis in commodities.jpg\n",
      "15\n",
      "Scraping: https://openlibrary.org/works/OL19709919W?edition=ia%3Arunnersbucketlis0000mala\n",
      "Downloaded image at: images/The runner's bucket list.jpg\n",
      "16\n",
      "Scraping: https://openlibrary.org/works/OL877225W?edition=ia%3Abasichomeremedie0000kush\n",
      "Downloaded image at: images/Basic home remedies.jpg\n",
      "17\n",
      "Scraping: https://openlibrary.org/works/OL23200W?edition=ia%3Amoonisdownnovel00stei\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = OpenLibraryScraper()\n",
    "    # book_urls =[\"https://openlibrary.org/works/OL41942014W\",\"https://openlibrary.org/works/OL15987761W?edition=ia%3Ablueumbrellanove0000maso\"]\n",
    "    # Example 1: Scrape specific books\n",
    "    df = scraper.scrape_books(book_urls[:20])\n",
    "    \n",
    "\n",
    "\n",
    "    scraper.save_to_csv(\"open_library_books.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
