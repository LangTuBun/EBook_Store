{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "PROBABILITIES = [0.005, 0.05, 0.1, 0.4, 0.445]\n",
    "\n",
    "def download_image(source, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        image_content = requests.get(source).content\n",
    "        f.write(image_content)\n",
    "        print('Downloaded image at:', filename)\n",
    "        \n",
    "\n",
    "class OpenLibraryScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.books_data = []\n",
    "        self.base_url = \"https://openlibrary.org\"\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"Make a request to the URL and return BeautifulSoup object\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "            return None\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean extracted text by removing extra whitespace and newlines\"\"\"\n",
    "        if text:\n",
    "            return re.sub(r'\\s+', ' ', text).strip()\n",
    "        return ''\n",
    "\n",
    "    def scrape_book(self, url):\n",
    "        \"\"\"Scrape individual book page from Open Library\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            book_data = {\n",
    "                'url': url,\n",
    "                'title': '',\n",
    "                'authors': [],\n",
    "                'publisher': '',\n",
    "                'publish_date': '',\n",
    "                'category': [],\n",
    "                'rating': '',\n",
    "                'page_count': '',\n",
    "                'languages': [],\n",
    "                'image_url': ''\n",
    "            }\n",
    "\n",
    "         \n",
    "\n",
    "            # Title\n",
    "            title_elem = soup.select_one('h1.work-title')\n",
    "            if title_elem:\n",
    "                book_data['title'] = self.clean_text(title_elem.text)\n",
    "            # Image URL\n",
    "            img_elem = soup.find('img', attrs = {'itemprop': 'image'})\n",
    "            img_url = 'https:' + img_elem['src'] if img_elem else None\n",
    "            valid_img = re.search(r\"openlibrary.org\", img_url)\n",
    "            if valid_img:\n",
    "                book_data['image_url'] = img_url\n",
    "                download_image(img_url, f\"images/{book_data['title']}.jpg\")\n",
    "            else:\n",
    "                print('No image found for:', book_data['title'])\n",
    "                return None\n",
    "            rating = random.choices([1,2,3,4,5], PROBABILITIES, k=1)[0]\n",
    "            book_data['rating'] = rating\n",
    "            # Authors\n",
    "            author_elems = soup.select('a[href^=\"/author\"]')\n",
    "            book_data['authors'] = list(set([self.clean_text(author.text) for author in author_elems]))\n",
    "\n",
    "            # Publisher and publish date\n",
    "            publish_info = soup.select('div.edition-omniline')\n",
    "            if publish_info:\n",
    "                pub_text = publish_info[0].text\n",
    "                pub_text = self.clean_text(pub_text)\n",
    "                publish_date_pattern = r\"Publish Date (\\w* \\d{1,2}, )?(?P<publish_date>\\d{4})\"\n",
    "\n",
    "                publisher_pattern = r\"Publisher (?P<publisher>.*) [Language]?\"  # Extract publisher\n",
    "                pages_pattern = r\"Pages (?P<pages>\\d+)\"  # Extract pages\n",
    "                language_pattern = r\"Language (?P<language>[^\\s]+)\"  # Extract language\n",
    "                # Extract publisher\n",
    "                # match = re.search(pattern, pub_text)\n",
    "                publisher_match = re.search(publisher_pattern, pub_text).group('publisher') if re.search(publisher_pattern, pub_text) else None\n",
    "                date_match = re.search(publish_date_pattern, pub_text).group('publish_date') if re.search(publish_date_pattern, pub_text) else None\n",
    "                pages_match = re.search(pages_pattern, pub_text).group('pages') if re.search(pages_pattern, pub_text) else None\n",
    "                language_match = re.search(language_pattern, pub_text).group('language') if re.search(language_pattern, pub_text) else None \n",
    "                if publisher_match:\n",
    "                    book_data['publisher'] = self.clean_text(publisher_match)\n",
    "                if date_match:\n",
    "                    book_data['publish_date'] = date_match\n",
    "                if language_match:\n",
    "                    book_data['languages'] = language_match\n",
    "                if pages_match:\n",
    "                    book_data['page_count'] = pages_match     \n",
    "\n",
    "            # category\n",
    "            subject_elems = soup.select('a[href^=\"/subjects/\"]')\n",
    "            book_data['category'] = [self.clean_text(subject.text) for subject in subject_elems]\n",
    "\n",
    "            # Description\n",
    "            desc_elem = soup.select_one('div[itemprop=\"description\"]')\n",
    "            if desc_elem:\n",
    "                book_data['description'] = self.clean_text(desc_elem.text)\n",
    "\n",
    "            # Page count\n",
    "            pages_elem = soup.select_one('span[itemprop=\"numberOfPages\"]')\n",
    "            if pages_elem:\n",
    "                book_data['page_count'] = self.clean_text(pages_elem.text)\n",
    "\n",
    "\n",
    "        \n",
    "            self.books_data.append(book_data)\n",
    "            return book_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping book {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def search_books(self, query, max_results=5):\n",
    "        \"\"\"Search for books and return their URLs\"\"\"\n",
    "        search_url = f\"{self.base_url}/search?q={query.replace(' ', '+')}&mode=everything\"\n",
    "        soup = self.get_soup(search_url)\n",
    "        if not soup:\n",
    "            return []\n",
    "\n",
    "        book_links = []\n",
    "        results = soup.select('h3.booktitle a[href^=\"/works/\"]')\n",
    "        \n",
    "        for result in results[:max_results]:\n",
    "            book_url = self.base_url + result['href']\n",
    "            book_links.append(book_url)\n",
    "            \n",
    "        return book_links\n",
    "\n",
    "    def scrape_books(self, urls):\n",
    "        \"\"\"Scrape multiple book pages\"\"\"\n",
    "        i = 0\n",
    "        for url in urls:\n",
    "            i+=1\n",
    "            print(i)\n",
    "            print(f\"Scraping: {url}\")\n",
    "            self.scrape_book(url)\n",
    "            sleep(2)  # Be nice to the server\n",
    "        \n",
    "        return self.export_to_dataframe()\n",
    "\n",
    "    def export_to_dataframe(self):\n",
    "        \"\"\"Export scraped data to pandas DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.books_data)\n",
    "\n",
    "    def save_to_csv(self, filename=\"books_data.csv\"):\n",
    "        \"\"\"Save scraped data to CSV file\"\"\"\n",
    "        df = self.export_to_dataframe()\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://openlibrary.org/works/OL25421708W', 'https://openlibrary.org/works/OL18593468W?edition=ia%3Afriend0000hill', 'https://openlibrary.org/works/OL3098719W?edition=ia%3Acompletedistance0000tull', 'https://openlibrary.org/works/OL17364743W?edition=ia%3Ahiddenoracle0000rior_l5g3', 'https://openlibrary.org/works/OL16808392W?edition=ia%3Ahouseofhades0000rior_f4t9']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"python_book_urls.csv\")\n",
    "book_urls = df['url'].tolist()\n",
    "print(book_urls[:5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Scraping: https://openlibrary.org/works/OL18738896W?edition=ia%3Awhirlingcircleso0000alle\n",
      "Downloaded image at: images/The Whirling Circles of Ba Gua Zhang.jpg\n",
      "2\n",
      "Scraping: https://openlibrary.org/works/OL2679070W?edition=ia%3Aopeningenergygat0000fran\n",
      "Downloaded image at: images/Opening the energy gates of your body.jpg\n",
      "3\n",
      "Scraping: https://openlibrary.org/works/OL17452319W?edition=ia%3Adragontigermedic0000fran\n",
      "Downloaded image at: images/Dragon And Tiger Medical Qigong Health And Energy In Seven Simple Movements.jpg\n",
      "4\n",
      "Scraping: https://openlibrary.org/works/OL3411239W?edition=ia%3Abarefootrunnerli0000ramb\n",
      "Downloaded image at: images/Barefoot Runner.jpg\n",
      "5\n",
      "Scraping: https://openlibrary.org/works/OL16804289W?edition=ia%3Abegynnelse0000danb\n",
      "Downloaded image at: images/Begynnelse.jpg\n",
      "6\n",
      "Scraping: https://openlibrary.org/works/OL34199971W\n",
      "No image found for: Ask for Andrea\n",
      "7\n",
      "Scraping: https://openlibrary.org/works/OL23286131W?edition=ia%3Aofficialrocknrol0000frai\n",
      "Downloaded image at: images/The official Rock'n'Roll guide to marathon & half-marathon training.jpg\n",
      "8\n",
      "Scraping: https://openlibrary.org/works/OL1825745W?edition=ia%3Amarathontraining0000hend\n",
      "Downloaded image at: images/Marathon training.jpg\n",
      "9\n",
      "Scraping: https://openlibrary.org/works/OL17006881W?edition=ia%3Aabundancecourse00cran\n",
      "Downloaded image at: images/The abundance course.jpg\n",
      "10\n",
      "Scraping: https://openlibrary.org/works/OL24679526W?edition=ia%3Arunningfrommiddl0000palm\n",
      "Downloaded image at: images/Running from middle distance to marathon.jpg\n",
      "11\n",
      "Scraping: https://openlibrary.org/works/OL22004799W?edition=ia%3Adjeliya0000juni\n",
      "Downloaded image at: images/Djeliya.jpg\n",
      "12\n",
      "Scraping: https://openlibrary.org/works/OL3255922W?edition=ia%3Atradinginchoppym0000barn\n",
      "Downloaded image at: images/Trading in Choppy Markets.jpg\n",
      "13\n",
      "Scraping: https://openlibrary.org/works/OL39181449W\n",
      "No image found for: Official CompTIA Network+ Study Guide (多多多多 N10- 009)\n",
      "14\n",
      "Scraping: https://openlibrary.org/works/OL8315358W?edition=ia%3Aisbn_0373348428\n",
      "Downloaded image at: images/Apprentie comtesse.jpg\n",
      "15\n",
      "Scraping: https://openlibrary.org/works/OL23060217W?edition=ia%3Acompleterunningm0000unse\n",
      "Downloaded image at: images/Complete Running and Marathon Book.jpg\n",
      "16\n",
      "Scraping: https://openlibrary.org/works/OL14958473W?edition=ia%3Atobemilitarysnip0000mast\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = OpenLibraryScraper()\n",
    "    # book_urls =[\"https://openlibrary.org/works/OL41942014W\",\"https://openlibrary.org/works/OL15987761W?edition=ia%3Ablueumbrellanove0000maso\"]\n",
    "    # Example 1: Scrape specific books\n",
    "    df = scraper.scrape_books(book_urls[20:40])\n",
    "    \n",
    "\n",
    "\n",
    "    scraper.save_to_csv(\"open_library_books.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
